{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Heat eqn 2.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "086221cf0dbfdd6f28656a5918bac9072f8e620a65fde2e6bda7f3ee0b75ad5a"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.10 64-bit ('juptenflowgpu115': conda)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "orig_nbformat": 2
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "__NAME = 'Heat 2'\n",
        "# root_path = '/content/drive/MyDrive/Colab Notebooks/Code/Heat eqn'\n",
        "# %tensorflow_version 1.x\n",
        "# !pip install pyDOE"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2j29WYr9ip1",
        "outputId": "a1ba87b9-1ae0-45ba-8b4e-d8c9aaac5f3c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzg4-93azs7D",
        "outputId": "60535a6a-b212-4b04-a043-00fcddf402de"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "# import os\n",
        "# os.chdir(root_path)\n",
        "# !pwd"
      ],
      "outputs": [],
      "metadata": {
        "id": "7cI8o_xiwixj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29aa7c5a-ca8a-412c-a205-4a6a8c0812b9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "Lo7Io-td9ip5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "import tensorflow as tf\r\n",
        "print(tf.version)\r\n",
        "print(tf.test.is_built_with_cuda())\r\n",
        "print(tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<module 'tensorflow._api.v1.version' from 'I:\\\\Users\\\\Ragav\\\\miniconda3\\\\envs\\\\juptenflowgpu115\\\\lib\\\\site-packages\\\\tensorflow_core\\\\_api\\\\v1\\\\version\\\\__init__.py'>\n",
            "True\n",
            "True\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ap_7gj5B9ip5",
        "outputId": "774a7e0c-bedb-4f31-ec4f-cc348f1142dc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "from mpl_toolkits.mplot3d import Axes3D\r\n",
        "# from plotting import newfig, savefig\r\n",
        "import matplotlib.gridspec as gridspec\r\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\r\n",
        "from matplotlib.animation import FuncAnimation, PillowWriter"
      ],
      "outputs": [],
      "metadata": {
        "id": "MZ_3z1QQ9ip6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "import numpy as np\r\n",
        "import scipy.io\r\n",
        "from scipy.interpolate import griddata\r\n",
        "import time\r\n",
        "from pyDOE import lhs"
      ],
      "outputs": [],
      "metadata": {
        "id": "G98vca_m9ip6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "import pickle as pkl"
      ],
      "outputs": [],
      "metadata": {
        "id": "o3Hv_tTnwltp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "%matplotlib widget"
      ],
      "outputs": [],
      "metadata": {
        "id": "16AVcexV9ip8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Equation"
      ],
      "metadata": {
        "id": "HJeh_z0KwnXv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "k = 0.061644"
      ],
      "outputs": [],
      "metadata": {
        "id": "yCKE0TXFwwyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\frac{\\partial u}{\\partial t} = k \\frac{\\partial^2 u}{\\partial x^2}\n",
        "$$"
      ],
      "metadata": {
        "id": "g6txgkLrwrDC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "HS9YWP019ip9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "mLzjyslO9ip-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "# Layers\r\n",
        "u_layers = [2, 50, 50, 50, 50, 1]\r\n",
        "pde_layers = [3, 100, 100, 1]\r\n",
        "\r\n",
        "layers = [2, 50, 50, 50, 50, 1]"
      ],
      "outputs": [],
      "metadata": {
        "id": "hnkSbtja9ip_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "# tf placeholders for Identification\r\n",
        "t_tf = tf.placeholder(tf.float32, shape=[None, 1])\r\n",
        "x_tf = tf.placeholder(tf.float32, shape=[None, 1])\r\n",
        "u_tf = tf.placeholder(tf.float32, shape=[None, 1])\r\n",
        "t_tf, x_tf, u_tf"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor 'Placeholder:0' shape=(?, 1) dtype=float32>,\n",
              " <tf.Tensor 'Placeholder_1:0' shape=(?, 1) dtype=float32>,\n",
              " <tf.Tensor 'Placeholder_2:0' shape=(?, 1) dtype=float32>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQIpj_2A9ip_",
        "outputId": "e2397671-91a9-48ff-cc07-3cbaf770f42d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "def initialize_NN(layers):\r\n",
        "    weights = []\r\n",
        "    biases = []\r\n",
        "    num_layers = len(layers)\r\n",
        "    for l in range(0, num_layers - 1):\r\n",
        "        W = xavier_init(size=[layers[l], layers[l + 1]])\r\n",
        "        b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32),\r\n",
        "                        dtype=tf.float32)\r\n",
        "        weights.append(W)\r\n",
        "        biases.append(b)\r\n",
        "    return weights, biases\r\n",
        "\r\n",
        "def xavier_init(size):\r\n",
        "    in_dim = size[0]\r\n",
        "    out_dim = size[1]\r\n",
        "    xavier_stddev = np.sqrt(2 / (in_dim + out_dim))\r\n",
        "    return tf.Variable(tf.truncated_normal([in_dim, out_dim],\r\n",
        "                                           stddev=xavier_stddev,\r\n",
        "                                           dtype=tf.float32),\r\n",
        "                       dtype=tf.float32)"
      ],
      "outputs": [],
      "metadata": {
        "id": "YuF7xZlf9iqA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "def neural_net(X, weights, biases):\r\n",
        "    num_layers = len(weights) + 1\r\n",
        "    H = X\r\n",
        "    for l in range(0, num_layers - 2):\r\n",
        "        W = weights[l]\r\n",
        "        b = biases[l]\r\n",
        "        H = tf.sin(tf.add(tf.matmul(H, W), b))\r\n",
        "    W = weights[-1]\r\n",
        "    b = biases[-1]\r\n",
        "    Y = tf.add(tf.matmul(H, W), b)\r\n",
        "    return Y"
      ],
      "outputs": [],
      "metadata": {
        "id": "LGD7-OeU9iqB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "weights, biases = initialize_NN(layers)\r\n",
        "# weights, biases"
      ],
      "outputs": [],
      "metadata": {
        "id": "GhGb8O9I9iqC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "# load weights and biases\r\n",
        "\r\n",
        "with open(__NAME + '/weights.pkl', 'rb') as db_file:\r\n",
        "    W_pkl = pkl.load(db_file)\r\n",
        "\r\n",
        "with open(__NAME + '/biases.pkl', 'rb') as db_file:\r\n",
        "    B_pkl = pkl.load(db_file)\r\n",
        "\r\n",
        "W = []\r\n",
        "B = []\r\n",
        "for w, b in zip(W_pkl, B_pkl):\r\n",
        "    W.append(tf.Variable(w))\r\n",
        "    B.append(tf.Variable(b))\r\n",
        "\r\n",
        "weights = W\r\n",
        "biases = B"
      ],
      "outputs": [],
      "metadata": {
        "id": "wyNbhRBy9iqD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "lb_tf = tf.placeholder(tf.float32, shape=[2])\r\n",
        "ub_tf = tf.placeholder(tf.float32, shape=[2])"
      ],
      "outputs": [],
      "metadata": {
        "id": "BtgYuufD9iqD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "# tf placeholders for Solution\r\n",
        "t0_tf = tf.placeholder(tf.float32, shape=[None, 1])\r\n",
        "x0_tf = tf.placeholder(tf.float32, shape=[None, 1])\r\n",
        "u0_tf = tf.placeholder(tf.float32, shape=[None, 1])\r\n",
        "\r\n",
        "t_lb_tf = tf.placeholder(tf.float32, shape=[None, 1])\r\n",
        "x_lb_tf = tf.placeholder(tf.float32, shape=[None, 1])\r\n",
        "t_ub_tf = tf.placeholder(tf.float32, shape=[None, 1])\r\n",
        "x_ub_tf = tf.placeholder(tf.float32, shape=[None, 1])\r\n",
        "u_x_ub_tf = tf.placeholder(tf.float32, shape=[None, 1])\r\n",
        "u_x_lb_tf = tf.placeholder(tf.float32, shape=[None, 1])\r\n",
        "\r\n",
        "t_f_tf = tf.placeholder(tf.float32, shape=[None, 1])\r\n",
        "x_f_tf = tf.placeholder(tf.float32, shape=[None, 1])"
      ],
      "outputs": [],
      "metadata": {
        "id": "ogiqxMk-9iqE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "def sol_net_u(t, x):\r\n",
        "    X = tf.concat([t, x], 1)\r\n",
        "    H = 2.0 * (X - lb_tf) / (ub_tf - lb_tf) - 1.0\r\n",
        "    u = neural_net(H, weights, biases)\r\n",
        "    u_x = tf.gradients(u, x)[0]\r\n",
        "    return u, u_x\r\n",
        "\r\n",
        "def sol_net_f(t, x):\r\n",
        "    u, u_x = sol_net_u(t, x)\r\n",
        "\r\n",
        "    u_t = tf.gradients(u, t)[0]\r\n",
        "\r\n",
        "    u_xx = tf.gradients(u_x, x)[0]\r\n",
        "\r\n",
        "    f = u_t - k * u_xx\r\n",
        "\r\n",
        "    return f"
      ],
      "outputs": [],
      "metadata": {
        "id": "Z1-Nl-Os9iqE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "source": [
        "# tf graphs for Solution\r\n",
        "u0_pred, u_x0_pred = sol_net_u(t0_tf, x0_tf)\r\n",
        "u_lb_pred, u_x_lb_pred = sol_net_u(t_lb_tf, x_lb_tf)\r\n",
        "u_ub_pred, u_x_ub_pred = sol_net_u(t_ub_tf, x_ub_tf)\r\n",
        "sol_f_pred = sol_net_f(t_f_tf, x_f_tf)"
      ],
      "outputs": [],
      "metadata": {
        "id": "bBDDLJa89iqE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "source": [
        "# loss for Solution\r\n",
        "sol_loss =  tf.reduce_sum(tf.square(u0_tf - u0_pred)) + \\\r\n",
        "            tf.reduce_sum(tf.square(u_x_lb_tf - u_x_lb_pred)) + \\\r\n",
        "            tf.reduce_sum(tf.square(u_x_ub_tf - u_x_ub_pred)) + \\\r\n",
        "            tf.reduce_sum(tf.square(sol_f_pred))"
      ],
      "outputs": [],
      "metadata": {
        "id": "gtwK9j1uweG3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "source": [
        "# Optimizer for Solution\r\n",
        "sol_optimizer = tf.contrib.opt.ScipyOptimizerInterface(\r\n",
        "    sol_loss,\r\n",
        "    var_list = weights + biases,\r\n",
        "    method='L-BFGS-B',\r\n",
        "    options={\r\n",
        "        'maxiter': 50000,\r\n",
        "        'maxfun': 50000,\r\n",
        "        'maxcor': 50,\r\n",
        "        'maxls': 50,\r\n",
        "        'ftol': 1.0 * np.finfo(float).eps\r\n",
        "    })"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSoiIsmr9iqE",
        "outputId": "930dcf65-c5a9-41ce-828b-3f6174b518cc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "source": [
        "adam_optimizer = tf.train.AdamOptimizer()\r\n",
        "sol_train_op_Adam = adam_optimizer.minimize(\r\n",
        "            sol_loss,\r\n",
        "            var_list= weights + biases)"
      ],
      "outputs": [],
      "metadata": {
        "id": "bvYabC3f9iqF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "source": [
        "# tf session\r\n",
        "sess = tf.Session(config=tf.ConfigProto(\r\n",
        "    allow_soft_placement=True, log_device_placement=True))\r\n",
        "init = tf.global_variables_initializer()\r\n",
        "sess.run(init)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnn8ivdu9iqF",
        "outputId": "ae07409a-a980-412b-b7c4-0a40933a5ab2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "1cm_gTGU9iqF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare data"
      ],
      "metadata": {
        "id": "lrmz1CKr9iqF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "source": [
        "lb = np.array([0.0, 0.0])\r\n",
        "ub = np.array([2.0, 1.0])"
      ],
      "outputs": [],
      "metadata": {
        "id": "AIxcc5ha9iqG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "source": [
        "N = 10\r\n",
        "fig = plt.figure()\r\n",
        "ax = fig.gca()\r\n",
        "ax.set_xlim(lb[0], ub[0])\r\n",
        "ax.set_ylim(lb[1], ub[1])\r\n",
        "ax.set_xticks(np.arange(lb[0],ub[0],(ub[0] - lb[0])/N))\r\n",
        "ax.set_yticks(np.arange(lb[1],ub[1],(ub[1] - lb[1])/N))\r\n",
        "plt.grid()\r\n",
        "l = lb + (ub - lb) * lhs(2, N) \r\n",
        "plt.scatter(l[:, 0], l[:, 1], color=\"r\", label=\"lhs\")\r\n",
        "plt.title(\"Latin Hypercube Sampling\\nN=10\")\r\n",
        "ax.set_xlabel('$t$')\r\n",
        "ax.set_ylabel('$x$')\r\n",
        "fig.set_figheight(3.8)\r\n",
        "fig.set_figwidth(6)\r\n",
        "plt.tight_layout()\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66a2b68eb7144ca2b854cf5fe2d07dc0"
            }
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "Cexgid3t9ip8",
        "outputId": "443d8d1e-088f-476e-8dae-6442a1244c54"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "source": [
        "N0 = 200\r\n",
        "N_b = 300\r\n",
        "N_f = 20000\r\n",
        "(N0, N_b, N_f)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200, 300, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENU0pYu49iqG",
        "outputId": "22d1910d-c456-4f4a-afbe-4576384d8588"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "source": [
        "t_data = np.linspace(lb[0], ub[0], N_b)[:, None]\r\n",
        "x_data = np.linspace(lb[1], ub[1], N0)[:, None]"
      ],
      "outputs": [],
      "metadata": {
        "id": "x_rIn6FT9iqG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "source": [
        "T_sol, X_sol = np.meshgrid(t_data, x_data)\r\n",
        "# U_sol = u_data"
      ],
      "outputs": [],
      "metadata": {
        "id": "-exIbLlM9iqH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "source": [
        "X_sol_star = np.hstack(\r\n",
        "    (T_sol.flatten()[:, None],\r\n",
        "    X_sol.flatten()[:, None])\r\n",
        "    )\r\n",
        "# U_sol_star = U_sol.flatten()[:, None]\r\n",
        "\r\n",
        "print(X_sol_star.shape, X_sol_star[0:10], sep='\\n')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 2)\n",
            "[[0.         0.        ]\n",
            " [0.00668896 0.        ]\n",
            " [0.01337793 0.        ]\n",
            " [0.02006689 0.        ]\n",
            " [0.02675585 0.        ]\n",
            " [0.03344482 0.        ]\n",
            " [0.04013378 0.        ]\n",
            " [0.04682274 0.        ]\n",
            " [0.05351171 0.        ]\n",
            " [0.06020067 0.        ]]\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c2OhmTk9iqH",
        "outputId": "f88e3ba3-6a49-406d-b989-96f7da80d401"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "source": [
        "L = 1"
      ],
      "outputs": [],
      "metadata": {
        "id": "GQWBiyOb9iqI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "source": [
        "u_data = np.sin(np.pi * x_data / L)"
      ],
      "outputs": [],
      "metadata": {
        "id": "SfyLa0f5O3uB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "source": [
        "X0 = np.hstack((T_sol[:, 0:1], X_sol[:, 0:1])) # left boundary\r\n",
        "X_lb = np.hstack((T_sol[0:1, :].T, X_sol[0:1, :].T)) # lower boundary\r\n",
        "X_ub = np.hstack((T_sol[0:1, :].T, np.repeat(ub[1], t_data.shape[0])[:, None])) # upper boundary"
      ],
      "outputs": [],
      "metadata": {
        "id": "LGRQThqW9iqH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "source": [
        "# shuffled initial boundary data (left boundary)\r\n",
        "idx_x = np.random.choice(x_data.shape[0], N0, replace=False)\r\n",
        "X0_train = X0[idx_x, :]\r\n",
        "u0_train = u_data[idx_x, 0:1]"
      ],
      "outputs": [],
      "metadata": {
        "id": "m3nwJ1z79iqH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "source": [
        "# shuffle time data\r\n",
        "idx_t = np.random.choice(t_data.shape[0], N_b, replace=False)\r\n",
        "tb_train = t_data[idx_t, :]"
      ],
      "outputs": [],
      "metadata": {
        "id": "I1gf-V_I9iqH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "source": [
        "X_f_train = lb + (ub - lb) * lhs(2, N_f) "
      ],
      "outputs": [],
      "metadata": {
        "id": "38DnkGLG9iqI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "source": [
        "fig, ax = plt.subplots()\r\n",
        "ax.set_xlim(lb[0] -0.1, ub[0])\r\n",
        "ax.set_ylim(lb[1] - 0.4, ub[1] + 0.4)\r\n",
        "fig.set_figheight(3.2)\r\n",
        "fig.set_figwidth(6)\r\n",
        "\r\n",
        "ax.scatter(X0_train[:, 0], X0_train[:, 1], s=4, marker='.')\r\n",
        "ax.scatter(tb_train[:, 0], np.repeat(lb[1], N_b), s=4, marker='.')\r\n",
        "ax.scatter(tb_train[:, 0], np.repeat(ub[1], N_b), s=4, marker='.')\r\n",
        "ax.scatter(X_f_train[:, 0], X_f_train[:, 1], s=4, marker='.', edgecolors='none')\r\n",
        "# ax.imshow(u0_train, extent=(t_data[0, 0], t_data[1, 0], x0_train.max(), x0_train.min()), aspect='auto')\r\n",
        "plt.tight_layout()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f795013dd1b648cd975e330748d29688"
            }
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "78wcudF49iqI",
        "outputId": "6224e70d-7369-47ed-ad15-5b582c0d9bd2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "source": [
        "fig, ax = plt.subplots()\r\n",
        "ax.set_xlabel('$x$')\r\n",
        "ax.set_ylabel('$u$')\r\n",
        "ax.plot(x_data[:, 0], u_data[:, 0:1])\r\n",
        "fig.set_figheight(3.2)\r\n",
        "fig.set_figwidth(6)\r\n",
        "plt.tight_layout()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd2cb0e632934fc88d5ba69e111f129c"
            }
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "qE20ahZW9iqJ",
        "outputId": "2b4bf679-418d-4cb9-9881-ecd4b40073e0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "source": [
        "def callback(loss):\r\n",
        "    print('Loss: %e' % (loss))"
      ],
      "outputs": [],
      "metadata": {
        "id": "hd3FNDpa9iqJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "source": [
        "tf_dict = {\r\n",
        "    lb_tf: lb,\r\n",
        "    ub_tf: ub,\r\n",
        "    t0_tf: X0_train[:, 0:1],\r\n",
        "    x0_tf: X0_train[:, 1:2],\r\n",
        "    u0_tf: u0_train,\r\n",
        "    t_lb_tf: X_lb[:, 0:1],\r\n",
        "    x_lb_tf: X_lb[:, 1:2],\r\n",
        "    t_ub_tf: X_ub[:, 0:1],\r\n",
        "    x_ub_tf: X_ub[:, 1:2],\r\n",
        "    u_x_lb_tf: np.repeat(0, N_b)[:, None],\r\n",
        "    u_x_ub_tf: np.repeat(0, N_b)[:, None],\r\n",
        "    t_f_tf: X_f_train[:, 0:1],\r\n",
        "    x_f_tf: X_f_train[:, 1:2]\r\n",
        "}"
      ],
      "outputs": [],
      "metadata": {
        "id": "k8rdO7oQ9iqJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "source": [
        "fig, ax = plt.subplots()\r\n",
        "ax.set_xlabel('$x$')\r\n",
        "ax.set_ylabel('$u$')\r\n",
        "ax.scatter(X0_train[:, 1:2], u0_train)\r\n",
        "fig.set_figheight(3.2)\r\n",
        "fig.set_figwidth(6)\r\n",
        "plt.tight_layout()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f837841fdad745679c5740eee490cdb2"
            }
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "UCIcqOP-weG-",
        "outputId": "423e768c-50de-4f98-f9d8-e20a8b86bdfc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "source": [
        "start_time = time.time()\r\n",
        "it = 0\r\n",
        "end = False\r\n",
        "while not(end):\r\n",
        "\r\n",
        "    sess.run(sol_train_op_Adam, tf_dict)\r\n",
        "\r\n",
        "    # Print\r\n",
        "    if it % 10 == 0:\r\n",
        "        elapsed = time.time() - start_time\r\n",
        "        loss_value = sess.run(sol_loss, tf_dict)\r\n",
        "        print('It: %d, Loss: %.3e, Time: %.2f' %\r\n",
        "                (it, loss_value, elapsed))\r\n",
        "        start_time = time.time()\r\n",
        "\r\n",
        "        if loss_value < 5 * 10**(-1):\r\n",
        "            end = True\r\n",
        "\r\n",
        "    it = it + 1"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It: 0, Loss: 1.102e+03, Time: 2.95\n",
            "It: 10, Loss: 5.190e+01, Time: 0.28\n",
            "It: 20, Loss: 7.309e+01, Time: 0.27\n",
            "It: 30, Loss: 1.274e+01, Time: 0.26\n",
            "It: 40, Loss: 5.873e+00, Time: 0.27\n",
            "It: 50, Loss: 4.257e+00, Time: 0.26\n",
            "It: 60, Loss: 2.421e+00, Time: 0.26\n",
            "It: 70, Loss: 1.312e+00, Time: 0.26\n",
            "It: 80, Loss: 9.169e-01, Time: 0.26\n",
            "It: 90, Loss: 7.219e-01, Time: 0.26\n",
            "It: 100, Loss: 6.346e-01, Time: 0.26\n",
            "It: 110, Loss: 5.742e-01, Time: 0.27\n",
            "It: 120, Loss: 5.237e-01, Time: 0.26\n",
            "It: 130, Loss: 4.869e-01, Time: 0.26\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPDDkBu89iqJ",
        "outputId": "13268e9a-5489-406d-a5b7-f52d75a900a8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "source": [
        "sol_optimizer.minimize(sess,\r\n",
        "                       feed_dict=tf_dict,\r\n",
        "                       fetches=[sol_loss],\r\n",
        "                       loss_callback=callback)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 4.869232e-01\n",
            "Loss: 1.365872e+05\n",
            "Loss: 4.860751e-01\n",
            "Loss: 4.855388e-01\n",
            "Loss: 4.849139e-01\n",
            "Loss: 4.835548e-01\n",
            "Loss: 4.803027e-01\n",
            "Loss: 4.724025e-01\n",
            "Loss: 4.589138e-01\n",
            "Loss: 4.365849e-01\n",
            "Loss: 4.000503e-01\n",
            "Loss: 3.601400e-01\n",
            "Loss: 3.413003e-01\n",
            "Loss: 3.341318e-01\n",
            "Loss: 3.278770e-01\n",
            "Loss: 3.250495e-01\n",
            "Loss: 3.210161e-01\n",
            "Loss: 3.154126e-01\n",
            "Loss: 3.083426e-01\n",
            "Loss: 3.093622e-01\n",
            "Loss: 3.053403e-01\n",
            "Loss: 3.017175e-01\n",
            "Loss: 2.979198e-01\n",
            "Loss: 2.935389e-01\n",
            "Loss: 2.879314e-01\n",
            "Loss: 2.845185e-01\n",
            "Loss: 2.806233e-01\n",
            "Loss: 2.784786e-01\n",
            "Loss: 2.748318e-01\n",
            "Loss: 2.690546e-01\n",
            "Loss: 2.702209e-01\n",
            "Loss: 2.668289e-01\n",
            "Loss: 2.643198e-01\n",
            "Loss: 2.629554e-01\n",
            "Loss: 2.612739e-01\n",
            "Loss: 2.584305e-01\n",
            "Loss: 2.595767e-01\n",
            "Loss: 2.571093e-01\n",
            "Loss: 2.553497e-01\n",
            "Loss: 2.539151e-01\n",
            "Loss: 2.518907e-01\n",
            "Loss: 2.482077e-01\n",
            "Loss: 2.426942e-01\n",
            "Loss: 2.398039e-01\n",
            "Loss: 2.361719e-01\n",
            "Loss: 2.347252e-01\n",
            "Loss: 2.328631e-01\n",
            "Loss: 2.286079e-01\n",
            "Loss: 2.234474e-01\n",
            "Loss: 2.195334e-01\n",
            "Loss: 2.152082e-01\n",
            "Loss: 2.117939e-01\n",
            "Loss: 2.044988e-01\n",
            "Loss: 1.991955e-01\n",
            "Loss: 1.958919e-01\n",
            "Loss: 1.936554e-01\n",
            "Loss: 1.909254e-01\n",
            "Loss: 1.872676e-01\n",
            "Loss: 1.825099e-01\n",
            "Loss: 1.789075e-01\n",
            "Loss: 1.760878e-01\n",
            "Loss: 1.734930e-01\n",
            "Loss: 1.713926e-01\n",
            "Loss: 1.699782e-01\n",
            "Loss: 1.693177e-01\n",
            "Loss: 1.682095e-01\n",
            "Loss: 1.639685e-01\n",
            "Loss: 1.607220e-01\n",
            "Loss: 1.561184e-01\n",
            "Loss: 1.546509e-01\n",
            "Loss: 1.532512e-01\n",
            "Loss: 1.511242e-01\n",
            "Loss: 1.485869e-01\n",
            "Loss: 1.465174e-01\n",
            "Loss: 1.452599e-01\n",
            "Loss: 1.443875e-01\n",
            "Loss: 1.431088e-01\n",
            "Loss: 1.413571e-01\n",
            "Loss: 1.385267e-01\n",
            "Loss: 1.360574e-01\n",
            "Loss: 1.343604e-01\n",
            "Loss: 1.328753e-01\n",
            "Loss: 1.318087e-01\n",
            "Loss: 1.302214e-01\n",
            "Loss: 1.285578e-01\n",
            "Loss: 1.279030e-01\n",
            "Loss: 1.267518e-01\n",
            "Loss: 1.264518e-01\n",
            "Loss: 1.260806e-01\n",
            "Loss: 1.255784e-01\n",
            "Loss: 1.248314e-01\n",
            "Loss: 1.238139e-01\n",
            "Loss: 1.229649e-01\n",
            "Loss: 1.221535e-01\n",
            "Loss: 1.214420e-01\n",
            "Loss: 1.201743e-01\n",
            "Loss: 1.199095e-01\n",
            "Loss: 1.183683e-01\n",
            "Loss: 1.176055e-01\n",
            "Loss: 1.166398e-01\n",
            "Loss: 1.158986e-01\n",
            "Loss: 1.148286e-01\n",
            "Loss: 1.140796e-01\n",
            "Loss: 1.128727e-01\n",
            "Loss: 1.123676e-01\n",
            "Loss: 1.114683e-01\n",
            "Loss: 1.111009e-01\n",
            "Loss: 1.107107e-01\n",
            "Loss: 1.100141e-01\n",
            "Loss: 1.088236e-01\n",
            "Loss: 1.082520e-01\n",
            "Loss: 1.076885e-01\n",
            "Loss: 1.074566e-01\n",
            "Loss: 1.072482e-01\n",
            "Loss: 1.078420e-01\n",
            "Loss: 1.070809e-01\n",
            "Loss: 1.067206e-01\n",
            "Loss: 1.062008e-01\n",
            "Loss: 1.066341e-01\n",
            "Loss: 1.059121e-01\n",
            "Loss: 1.053620e-01\n",
            "Loss: 1.041944e-01\n",
            "Loss: 1.039692e-01\n",
            "Loss: 1.032379e-01\n",
            "Loss: 1.030899e-01\n",
            "Loss: 1.028640e-01\n",
            "Loss: 1.027878e-01\n",
            "Loss: 1.025747e-01\n",
            "Loss: 1.024508e-01\n",
            "Loss: 1.022962e-01\n",
            "Loss: 1.020914e-01\n",
            "Loss: 1.017522e-01\n",
            "Loss: 1.014513e-01\n",
            "Loss: 1.008700e-01\n",
            "Loss: 1.008314e-01\n",
            "Loss: 1.006614e-01\n",
            "Loss: 1.003487e-01\n",
            "Loss: 9.995472e-02\n",
            "Loss: 9.955740e-02\n",
            "Loss: 9.938296e-02\n",
            "Loss: 9.880331e-02\n",
            "Loss: 9.857433e-02\n",
            "Loss: 9.810347e-02\n",
            "Loss: 9.776738e-02\n",
            "Loss: 9.748551e-02\n",
            "Loss: 9.697485e-02\n",
            "Loss: 9.802247e-02\n",
            "Loss: 9.684904e-02\n",
            "Loss: 9.658466e-02\n",
            "Loss: 9.627293e-02\n",
            "Loss: 9.592459e-02\n",
            "Loss: 9.551375e-02\n",
            "Loss: 9.550643e-02\n",
            "Loss: 9.530599e-02\n",
            "Loss: 9.517913e-02\n",
            "Loss: 9.508716e-02\n",
            "Loss: 9.501006e-02\n",
            "Loss: 9.485138e-02\n",
            "Loss: 9.460464e-02\n",
            "Loss: 9.444556e-02\n",
            "Loss: 9.415697e-02\n",
            "Loss: 9.403072e-02\n",
            "Loss: 9.386681e-02\n",
            "Loss: 9.374899e-02\n",
            "Loss: 9.357858e-02\n",
            "Loss: 9.343474e-02\n",
            "Loss: 9.332240e-02\n",
            "Loss: 9.318966e-02\n",
            "Loss: 9.309318e-02\n",
            "Loss: 9.301218e-02\n",
            "Loss: 9.292936e-02\n",
            "Loss: 9.284571e-02\n",
            "Loss: 9.275205e-02\n",
            "Loss: 9.263465e-02\n",
            "Loss: 9.249479e-02\n",
            "Loss: 9.237042e-02\n",
            "Loss: 9.201019e-02\n",
            "Loss: 9.239928e-02\n",
            "Loss: 9.186690e-02\n",
            "Loss: 9.159656e-02\n",
            "Loss: 9.137742e-02\n",
            "Loss: 9.114373e-02\n",
            "Loss: 9.086836e-02\n",
            "Loss: 9.064803e-02\n",
            "Loss: 9.052795e-02\n",
            "Loss: 9.047721e-02\n",
            "Loss: 9.039009e-02\n",
            "Loss: 9.021110e-02\n",
            "Loss: 9.000100e-02\n",
            "Loss: 8.990805e-02\n",
            "Loss: 8.972101e-02\n",
            "Loss: 8.964454e-02\n",
            "Loss: 8.954476e-02\n",
            "Loss: 8.938697e-02\n",
            "Loss: 8.929130e-02\n",
            "Loss: 8.910231e-02\n",
            "Loss: 8.899610e-02\n",
            "Loss: 8.888011e-02\n",
            "Loss: 8.870740e-02\n",
            "Loss: 8.844962e-02\n",
            "Loss: 8.854557e-02\n",
            "Loss: 8.830357e-02\n",
            "Loss: 8.814425e-02\n",
            "Loss: 8.802877e-02\n",
            "Loss: 8.796789e-02\n",
            "Loss: 8.785432e-02\n",
            "Loss: 8.778143e-02\n",
            "Loss: 8.774477e-02\n",
            "Loss: 8.768534e-02\n",
            "Loss: 8.767813e-02\n",
            "Loss: 8.763155e-02\n",
            "Loss: 8.754583e-02\n",
            "Loss: 8.749224e-02\n",
            "Loss: 8.741373e-02\n",
            "Loss: 8.731152e-02\n",
            "Loss: 8.725217e-02\n",
            "Loss: 8.711994e-02\n",
            "Loss: 8.702087e-02\n",
            "Loss: 8.693847e-02\n",
            "Loss: 8.680893e-02\n",
            "Loss: 8.669839e-02\n",
            "Loss: 8.660847e-02\n",
            "Loss: 8.652790e-02\n",
            "Loss: 8.646157e-02\n",
            "Loss: 8.637246e-02\n",
            "Loss: 8.621384e-02\n",
            "Loss: 8.614890e-02\n",
            "Loss: 8.604264e-02\n",
            "Loss: 8.593715e-02\n",
            "Loss: 8.580077e-02\n",
            "Loss: 8.572122e-02\n",
            "Loss: 8.562355e-02\n",
            "Loss: 8.558238e-02\n",
            "Loss: 8.581972e-02\n",
            "Loss: 8.556136e-02\n",
            "Loss: 8.550426e-02\n",
            "Loss: 8.545306e-02\n",
            "Loss: 8.540262e-02\n",
            "Loss: 8.545404e-02\n",
            "Loss: 8.537976e-02\n",
            "Loss: 8.534602e-02\n",
            "Loss: 8.527762e-02\n",
            "Loss: 8.564474e-02\n",
            "Loss: 8.523763e-02\n",
            "Loss: 8.516233e-02\n",
            "Loss: 8.507396e-02\n",
            "Loss: 8.500456e-02\n",
            "Loss: 8.496714e-02\n",
            "Loss: 8.489117e-02\n",
            "Loss: 8.483683e-02\n",
            "Loss: 8.478057e-02\n",
            "Loss: 8.469529e-02\n",
            "Loss: 8.470506e-02\n",
            "Loss: 8.462181e-02\n",
            "Loss: 8.451655e-02\n",
            "Loss: 8.438893e-02\n",
            "Loss: 8.425858e-02\n",
            "Loss: 8.402594e-02\n",
            "Loss: 8.451571e-02\n",
            "Loss: 8.396310e-02\n",
            "Loss: 8.382134e-02\n",
            "Loss: 8.372537e-02\n",
            "Loss: 8.359179e-02\n",
            "Loss: 8.346500e-02\n",
            "Loss: 8.347610e-02\n",
            "Loss: 8.337461e-02\n",
            "Loss: 8.329616e-02\n",
            "Loss: 8.321465e-02\n",
            "Loss: 8.315321e-02\n",
            "Loss: 8.301655e-02\n",
            "Loss: 8.388779e-02\n",
            "Loss: 8.298816e-02\n",
            "Loss: 8.290707e-02\n",
            "Loss: 8.283420e-02\n",
            "Loss: 8.272912e-02\n",
            "Loss: 8.262497e-02\n",
            "Loss: 8.248788e-02\n",
            "Loss: 8.237155e-02\n",
            "Loss: 8.230732e-02\n",
            "Loss: 8.226314e-02\n",
            "Loss: 8.220027e-02\n",
            "Loss: 8.210679e-02\n",
            "Loss: 8.205258e-02\n",
            "Loss: 8.198365e-02\n",
            "Loss: 8.194853e-02\n",
            "Loss: 8.191623e-02\n",
            "Loss: 8.188257e-02\n",
            "Loss: 8.183458e-02\n",
            "Loss: 8.181685e-02\n",
            "Loss: 8.178475e-02\n",
            "Loss: 8.196380e-02\n",
            "Loss: 8.177686e-02\n",
            "Loss: 8.175132e-02\n",
            "Loss: 8.173320e-02\n",
            "Loss: 8.169464e-02\n",
            "Loss: 8.167967e-02\n",
            "Loss: 8.166001e-02\n",
            "Loss: 8.162840e-02\n",
            "Loss: 8.158261e-02\n",
            "Loss: 8.155158e-02\n",
            "Loss: 8.152891e-02\n",
            "Loss: 8.150163e-02\n",
            "Loss: 8.149108e-02\n",
            "Loss: 8.146610e-02\n",
            "Loss: 8.146477e-02\n",
            "Loss: 8.144490e-02\n",
            "Loss: 8.143611e-02\n",
            "Loss: 8.142264e-02\n",
            "Loss: 8.140726e-02\n",
            "Loss: 8.139300e-02\n",
            "Loss: 8.137874e-02\n",
            "Loss: 8.135999e-02\n",
            "Loss: 8.134299e-02\n",
            "Loss: 8.132295e-02\n",
            "Loss: 8.131234e-02\n",
            "Loss: 8.129745e-02\n",
            "Loss: 8.128566e-02\n",
            "Loss: 8.125971e-02\n",
            "Loss: 8.124667e-02\n",
            "Loss: 8.123217e-02\n",
            "Loss: 8.122090e-02\n",
            "Loss: 8.120316e-02\n",
            "Loss: 8.117963e-02\n",
            "Loss: 8.115563e-02\n",
            "Loss: 8.114049e-02\n",
            "Loss: 8.112520e-02\n",
            "Loss: 8.111537e-02\n",
            "Loss: 8.109757e-02\n",
            "Loss: 8.108087e-02\n",
            "Loss: 8.105865e-02\n",
            "Loss: 8.107649e-02\n",
            "Loss: 8.105037e-02\n",
            "Loss: 8.104061e-02\n",
            "Loss: 8.103033e-02\n",
            "Loss: 8.101791e-02\n",
            "Loss: 8.100262e-02\n",
            "Loss: 8.098982e-02\n",
            "Loss: 8.097960e-02\n",
            "Loss: 8.096179e-02\n",
            "Loss: 8.094567e-02\n",
            "Loss: 8.091681e-02\n",
            "Loss: 8.101811e-02\n",
            "Loss: 8.090287e-02\n",
            "Loss: 8.088925e-02\n",
            "Loss: 8.087309e-02\n",
            "Loss: 8.086043e-02\n",
            "Loss: 8.083556e-02\n",
            "Loss: 8.083607e-02\n",
            "Loss: 8.082470e-02\n",
            "Loss: 8.081568e-02\n",
            "Loss: 8.080847e-02\n",
            "Loss: 8.079851e-02\n",
            "Loss: 8.078288e-02\n",
            "Loss: 8.076577e-02\n",
            "Loss: 8.074646e-02\n",
            "Loss: 8.073225e-02\n",
            "Loss: 8.071789e-02\n",
            "Loss: 8.070512e-02\n",
            "Loss: 8.067907e-02\n",
            "Loss: 8.065839e-02\n",
            "Loss: 8.063585e-02\n",
            "Loss: 8.062164e-02\n",
            "Loss: 8.061093e-02\n",
            "Loss: 8.060430e-02\n",
            "Loss: 8.059773e-02\n",
            "Loss: 8.059275e-02\n",
            "Loss: 8.057795e-02\n",
            "Loss: 8.057008e-02\n",
            "Loss: 8.055475e-02\n",
            "Loss: 8.054910e-02\n",
            "Loss: 8.054395e-02\n",
            "Loss: 8.053765e-02\n",
            "Loss: 8.057950e-02\n",
            "Loss: 8.053350e-02\n",
            "Loss: 8.052060e-02\n",
            "Loss: 8.051028e-02\n",
            "Loss: 8.050337e-02\n",
            "Loss: 8.049810e-02\n",
            "Loss: 8.048753e-02\n",
            "Loss: 8.047803e-02\n",
            "Loss: 8.046655e-02\n",
            "Loss: 8.046418e-02\n",
            "Loss: 8.045425e-02\n",
            "Loss: 8.045118e-02\n",
            "Loss: 8.044510e-02\n",
            "Loss: 8.043915e-02\n",
            "Loss: 8.043023e-02\n",
            "Loss: 8.042765e-02\n",
            "Loss: 8.041880e-02\n",
            "Loss: 8.041438e-02\n",
            "Loss: 8.040802e-02\n",
            "Loss: 8.040740e-02\n",
            "Loss: 8.039953e-02\n",
            "Loss: 8.039505e-02\n",
            "Loss: 8.038681e-02\n",
            "Loss: 8.037596e-02\n",
            "Loss: 8.036175e-02\n",
            "Loss: 8.036310e-02\n",
            "Loss: 8.035440e-02\n",
            "Loss: 8.034210e-02\n",
            "Loss: 8.033093e-02\n",
            "Loss: 8.032295e-02\n",
            "Loss: 8.031493e-02\n",
            "Loss: 8.030915e-02\n",
            "Loss: 8.029320e-02\n",
            "Loss: 8.028777e-02\n",
            "Loss: 8.027482e-02\n",
            "Loss: 8.026361e-02\n",
            "Loss: 8.025463e-02\n",
            "Loss: 8.024357e-02\n",
            "Loss: 8.023944e-02\n",
            "Loss: 8.022808e-02\n",
            "Loss: 8.022451e-02\n",
            "Loss: 8.022016e-02\n",
            "Loss: 8.021509e-02\n",
            "Loss: 8.021092e-02\n",
            "Loss: 8.020950e-02\n",
            "Loss: 8.020371e-02\n",
            "Loss: 8.020005e-02\n",
            "Loss: 8.019530e-02\n",
            "Loss: 8.018758e-02\n",
            "Loss: 8.017961e-02\n",
            "Loss: 8.018859e-02\n",
            "Loss: 8.017646e-02\n",
            "Loss: 8.017134e-02\n",
            "Loss: 8.016565e-02\n",
            "Loss: 8.016048e-02\n",
            "Loss: 8.015534e-02\n",
            "Loss: 8.015573e-02\n",
            "Loss: 8.015218e-02\n",
            "Loss: 8.014941e-02\n",
            "Loss: 8.014721e-02\n",
            "Loss: 8.014514e-02\n",
            "Loss: 8.014143e-02\n",
            "Loss: 8.015129e-02\n",
            "Loss: 8.013999e-02\n",
            "Loss: 8.013740e-02\n",
            "Loss: 8.013545e-02\n",
            "Loss: 8.013417e-02\n",
            "Loss: 8.013158e-02\n",
            "Loss: 8.013780e-02\n",
            "Loss: 8.013043e-02\n",
            "Loss: 8.012719e-02\n",
            "Loss: 8.012360e-02\n",
            "Loss: 8.012019e-02\n",
            "Loss: 8.011670e-02\n",
            "Loss: 8.011177e-02\n",
            "Loss: 8.010723e-02\n",
            "Loss: 8.010284e-02\n",
            "Loss: 8.009895e-02\n",
            "Loss: 8.009528e-02\n",
            "Loss: 8.009174e-02\n",
            "Loss: 8.009014e-02\n",
            "Loss: 8.008692e-02\n",
            "Loss: 8.008226e-02\n",
            "Loss: 8.007758e-02\n",
            "Loss: 8.007430e-02\n",
            "Loss: 8.007299e-02\n",
            "Loss: 8.007136e-02\n",
            "Loss: 8.006943e-02\n",
            "Loss: 8.006747e-02\n",
            "Loss: 8.006474e-02\n",
            "Loss: 8.006566e-02\n",
            "Loss: 8.006332e-02\n",
            "Loss: 8.006047e-02\n",
            "Loss: 8.005917e-02\n",
            "Loss: 8.005802e-02\n",
            "Loss: 8.005570e-02\n",
            "Loss: 8.005265e-02\n",
            "Loss: 8.004837e-02\n",
            "Loss: 8.004445e-02\n",
            "Loss: 8.004013e-02\n",
            "Loss: 8.003712e-02\n",
            "Loss: 8.003496e-02\n",
            "Loss: 8.003297e-02\n",
            "Loss: 8.003159e-02\n",
            "Loss: 8.003027e-02\n",
            "Loss: 8.002748e-02\n",
            "Loss: 8.002603e-02\n",
            "Loss: 8.002541e-02\n",
            "Loss: 8.002338e-02\n",
            "Loss: 8.002136e-02\n",
            "Loss: 8.001946e-02\n",
            "Loss: 8.001661e-02\n",
            "Loss: 8.001500e-02\n",
            "Loss: 8.000981e-02\n",
            "Loss: 8.001394e-02\n",
            "Loss: 8.000807e-02\n",
            "Loss: 8.000551e-02\n",
            "Loss: 8.000444e-02\n",
            "Loss: 8.000179e-02\n",
            "Loss: 8.000051e-02\n",
            "Loss: 7.999744e-02\n",
            "Loss: 7.999487e-02\n",
            "Loss: 7.999339e-02\n",
            "Loss: 7.999312e-02\n",
            "Loss: 7.999168e-02\n",
            "Loss: 7.999104e-02\n",
            "Loss: 7.998986e-02\n",
            "Loss: 7.998836e-02\n",
            "Loss: 7.998462e-02\n",
            "Loss: 7.998317e-02\n",
            "Loss: 7.998055e-02\n",
            "Loss: 7.997937e-02\n",
            "Loss: 7.997762e-02\n",
            "Loss: 7.997507e-02\n",
            "Loss: 7.997560e-02\n",
            "Loss: 7.997292e-02\n",
            "Loss: 7.996964e-02\n",
            "Loss: 7.996680e-02\n",
            "Loss: 7.996354e-02\n",
            "Loss: 7.995879e-02\n",
            "Loss: 7.996298e-02\n",
            "Loss: 7.995709e-02\n",
            "Loss: 7.995342e-02\n",
            "Loss: 7.995173e-02\n",
            "Loss: 7.995074e-02\n",
            "Loss: 7.994843e-02\n",
            "Loss: 7.994425e-02\n",
            "Loss: 7.994188e-02\n",
            "Loss: 7.993972e-02\n",
            "Loss: 7.993877e-02\n",
            "Loss: 7.993761e-02\n",
            "Loss: 7.993527e-02\n",
            "Loss: 7.993265e-02\n",
            "Loss: 7.992963e-02\n",
            "Loss: 7.992785e-02\n",
            "Loss: 7.992520e-02\n",
            "Loss: 7.992361e-02\n",
            "Loss: 7.992109e-02\n",
            "Loss: 7.992303e-02\n",
            "Loss: 7.991953e-02\n",
            "Loss: 7.991763e-02\n",
            "Loss: 7.991667e-02\n",
            "Loss: 7.991681e-02\n",
            "Loss: 7.991622e-02\n",
            "Loss: 7.991580e-02\n",
            "Loss: 7.991464e-02\n",
            "Loss: 7.991274e-02\n",
            "Loss: 7.991269e-02\n",
            "Loss: 7.991153e-02\n",
            "Loss: 7.990913e-02\n",
            "Loss: 7.990723e-02\n",
            "Loss: 7.990651e-02\n",
            "Loss: 7.990482e-02\n",
            "Loss: 7.990189e-02\n",
            "Loss: 7.989886e-02\n",
            "Loss: 7.989623e-02\n",
            "Loss: 7.989565e-02\n",
            "Loss: 7.989401e-02\n",
            "Loss: 7.989299e-02\n",
            "Loss: 7.989147e-02\n",
            "Loss: 7.988905e-02\n",
            "Loss: 7.988667e-02\n",
            "Loss: 7.988854e-02\n",
            "Loss: 7.988650e-02\n",
            "Loss: 7.988412e-02\n",
            "Loss: 7.988372e-02\n",
            "Loss: 7.988229e-02\n",
            "Loss: 7.988022e-02\n",
            "Loss: 7.987912e-02\n",
            "Loss: 7.987595e-02\n",
            "Loss: 7.987482e-02\n",
            "Loss: 7.987286e-02\n",
            "Loss: 7.987054e-02\n",
            "Loss: 7.986932e-02\n",
            "Loss: 7.986809e-02\n",
            "Loss: 7.986739e-02\n",
            "Loss: 7.986538e-02\n",
            "Loss: 7.987937e-02\n",
            "Loss: 7.986531e-02\n",
            "Loss: 7.986366e-02\n",
            "Loss: 7.986208e-02\n",
            "Loss: 7.986055e-02\n",
            "Loss: 7.985955e-02\n",
            "Loss: 7.985883e-02\n",
            "Loss: 7.985742e-02\n",
            "Loss: 7.985684e-02\n",
            "Loss: 7.985649e-02\n",
            "Loss: 7.985384e-02\n",
            "Loss: 7.985261e-02\n",
            "Loss: 7.985519e-02\n",
            "Loss: 7.985124e-02\n",
            "Loss: 7.985003e-02\n",
            "Loss: 7.984923e-02\n",
            "Loss: 7.984790e-02\n",
            "Loss: 7.984577e-02\n",
            "Loss: 7.984430e-02\n",
            "Loss: 7.984270e-02\n",
            "Loss: 7.984176e-02\n",
            "Loss: 7.984073e-02\n",
            "Loss: 7.984084e-02\n",
            "Loss: 7.984088e-02\n",
            "Loss: 7.984079e-02\n",
            "Loss: 7.984100e-02\n",
            "Loss: 7.984073e-02\n",
            "Loss: 7.984073e-02\n",
            "Loss: 7.984073e-02\n",
            "Loss: 7.984073e-02\n",
            "Loss: 7.984073e-02\n",
            "Loss: 7.984073e-02\n",
            "Loss: 7.984073e-02\n",
            "Loss: 7.984073e-02\n",
            "Loss: 7.984073e-02\n",
            "Loss: 7.984073e-02\n",
            "Loss: 7.984073e-02\n",
            "Loss: 7.984073e-02\n",
            "Loss: 7.984073e-02\n",
            "Loss: 7.984073e-02\n",
            "INFO:tensorflow:Optimization terminated with:\n",
            "  Message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
            "  Objective function value: 0.079841\n",
            "  Number of iterations: 552\n",
            "  Number of functions evaluations: 609\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFDrRRtB9iqK",
        "tags": [],
        "outputId": "65c13663-106b-44a4-cdc7-682afd521fdf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "source": [
        "sess.run(sol_loss, feed_dict=tf_dict)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.07984073"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iJD5gOT9iqL",
        "outputId": "722ad038-b2b9-4602-9675-e9879a8c7afe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "source": [
        "with open(__NAME + '/weights.pkl', 'wb') as db_file:\r\n",
        "    pkl.dump(obj=sess.run(weights), file=db_file)\r\n",
        "\r\n",
        "with open(__NAME + '/biases.pkl', 'wb') as db_file:\r\n",
        "    pkl.dump(obj=sess.run(biases), file=db_file)"
      ],
      "outputs": [],
      "metadata": {
        "id": "pUGVyB2EzP8o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "source": [
        "u_pred = sess.run(u0_pred, {\r\n",
        "        lb_tf: lb,\r\n",
        "        ub_tf: ub,\r\n",
        "        t0_tf: X_sol_star[:, 0:1],\r\n",
        "        x0_tf: X_sol_star[:, 1:2]\r\n",
        "    })"
      ],
      "outputs": [],
      "metadata": {
        "id": "0-Ao4DeA9iqL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "source": [
        "fig = plt.figure(figsize=(4*1.75,4), dpi=200)\r\n",
        "ax = fig.gca()\r\n",
        "\r\n",
        "ax.set_xlim(lb[0], ub[0])\r\n",
        "ax.set_ylim(lb[1], ub[1])\r\n",
        "\r\n",
        "# plt.subplots_adjust(bottom=0.17)\r\n",
        "# plt.subplots_adjust(left=0.17)\r\n",
        "\r\n",
        "plt.title('T')\r\n",
        "ax.set_xlabel('$t$')\r\n",
        "ax.set_ylabel('$x$')\r\n",
        "\r\n",
        "plt.pcolormesh(np.reshape(X_sol_star[:, 0], (N0, -1)), \r\n",
        "               np.reshape(X_sol_star[:, 1], (N0, -1)), \r\n",
        "               np.reshape(u_pred[:, 0], (N0, -1)), \r\n",
        "               shading='gouraud',  cmap='jet')\r\n",
        "plt.colorbar()\r\n",
        "\r\n",
        "plt.tight_layout()\r\n",
        "\r\n",
        "# plt.legend()\r\n",
        "\r\n",
        "fig.savefig('Figures\\\\Heat 2.png')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a77c7a9678f47e3b39f2ccf07458c74"
            }
          },
          "metadata": {}
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "source": [
        "t = np.reshape(X_sol_star[:, 0], (N0, -1))\r\n",
        "x = np.reshape(X_sol_star[:, 1], (N0, -1))\r\n",
        "u = np.reshape(u_pred[:, 0], (N0, -1))\r\n",
        "\r\n",
        "x_init = x[:, 0]\r\n",
        "u_init = u[:, 0]\r\n",
        "\r\n",
        "fig = plt.figure(figsize=(4*1.75,4), dpi=200)\r\n",
        "ax = fig.gca()\r\n",
        "\r\n",
        "ax.set_xlim(lb[1], ub[1])\r\n",
        "\r\n",
        "ax.yaxis.grid(color='gainsboro', linestyle='dotted', linewidth=1.5)\r\n",
        "ax.xaxis.grid(color='gainsboro', linestyle='dotted', linewidth=0.8)\r\n",
        "ax.axhline(0,linestyle='dotted', color='grey')\r\n",
        "ax.spines['right'].set_color('none')\r\n",
        "ax.spines['top'].set_color('none')\r\n",
        "\r\n",
        "plt.title('t = 0')\r\n",
        "\r\n",
        "ax.set_xlabel('$x$')\r\n",
        "ax.set_ylabel('$T$')\r\n",
        "\r\n",
        "plt.tight_layout()\r\n",
        "\r\n",
        "ln, = ax.plot(x_init, u_init)\r\n",
        "\r\n",
        "def update(frame):\r\n",
        "    plt.title('t = {time:.2f}'.format(time = t[0, frame]))\r\n",
        "    ln.set_data(x[:, frame], u[:, frame])\r\n",
        "\r\n",
        "ani = FuncAnimation(fig, update, list(range(0, N_b)))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4561c3e737c34dbb8fac330ac669f5cb"
            }
          },
          "metadata": {}
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "source": [
        "ani.event_source.stop()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "source": [
        "# anim.event_source.stop()\r\n",
        "writer = PillowWriter(fps=25) \r\n",
        "ani.save(\"Figures\\\\Heat 2.gif\", writer=writer)\r\n",
        "# ani.event_source.stop()"
      ],
      "outputs": [],
      "metadata": {}
    }
  ]
}